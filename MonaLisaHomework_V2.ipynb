{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 4\n",
    "\n",
    "SYS 6018: Machine Learning\n",
    "\n",
    "Caitlin Dreisbach (CND2y)\n",
    "\n",
    "Elizabeth Homan Harrsion (EIH2NN)\n",
    "\n",
    "Morgan Wall (MKY2K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Functions:\n",
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x,name):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME',name=name)\n",
    " \n",
    "\n",
    "def weight_variable(shape,varname):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=varname)\n",
    "\n",
    "\n",
    "def bias_variable(shape,varname):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial, name=varname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ConvNet\n",
    "def deepnn(x):\n",
    "    # x is 64x64x3\n",
    "    \n",
    "    # First Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([5, 5, 3, 16],'conv_w')\n",
    "        b_conv1 = bias_variable([16],'conv_b')\n",
    "        h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1,name='h_conv_op')\n",
    "        h_pool1 = max_pool_2x2(h_conv1,'h_pool_op')\n",
    "    # output: 32x32x16\n",
    " \n",
    "    # Second Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([5, 5, 16, 16],'conv_w')\n",
    "        b_conv2 = bias_variable([16],'conv_b')\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2,name='h_conv_op')\n",
    "        h_pool2 = max_pool_2x2(h_conv2,'h_pool_op')\n",
    "    # output: 16x16x16\n",
    "\n",
    "    # Third Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv3'):\n",
    "        W_conv3 = weight_variable([5, 5, 16, 32],'conv_w')\n",
    "        b_conv3 = bias_variable([32],'conv_b')\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3,name='h_conv_op')\n",
    "        h_pool3 = max_pool_2x2(h_conv3,'h_pool_op')\n",
    "    # output: 8x8x32\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    with tf.name_scope('fc'):\n",
    "        # Flatten output from previous layer\n",
    "        h_pool3_flat = tf.reshape(h_pool3, [-1, 8*8*32])\n",
    "        # Fully connected + Relu\n",
    "        # input: flat (8x8x32) = 1x2048\n",
    "        W_fc1 = weight_variable([8 * 8* 32, 1024],'fc_w')\n",
    "        b_fc1 = bias_variable([1024],'fc_b')\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "    # output: 1x1024\n",
    "    \n",
    "    # Dropout\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Output Softmax Layer: Map the 1024 features to 2 classes, one for each class type\n",
    "    with tf.name_scope('sm'):\n",
    "        W_sm = weight_variable([1024, 2],'sm_w')\n",
    "        b_sm = bias_variable([2],'sm_b')\n",
    "        y_conv_out = tf.add(tf.matmul(h_fc1_drop, W_sm), b_sm, name=\"y_conv_op\")\n",
    " \n",
    "    return y_conv_out,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra functions:\n",
    "# DataSet class to handle images \n",
    "class DataSet(object):\n",
    "    def __init__(self, images, labels):\n",
    "        self._num_examples = len(images)\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "        self._epochs_done = 0\n",
    "        self._index_in_epoch = 0\n",
    "        np.random.seed(123456)\n",
    "        # Shuffle the data\n",
    "        perm = np.arange(self._num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        self._images = self._images[perm]\n",
    "        self._labels = self._labels[perm]\n",
    "        random.seed(123456)\n",
    "    @property\n",
    "    def images(self):\n",
    "        return self._images\n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels\n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    @property\n",
    "    def epochs_done(self):\n",
    "        return self._epochs_done\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"Return the next `batch_size` examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "\n",
    "        if self._index_in_epoch > self._num_examples:\n",
    "            # After each epoch we update this\n",
    "            self._epochs_done += 1\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self._num_examples)\n",
    "            np.random.shuffle(perm)\n",
    "            self._images = self._images[perm]\n",
    "            self._labels = self._labels[perm] \n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._num_examples\n",
    "        end = self._index_in_epoch\n",
    "    \n",
    "        return self._images[start:end], self._labels[start:end]\n",
    "\n",
    "# read images from a directory\n",
    "def read_images(path,images,labels,class_num,class_index):\n",
    "    files = glob.glob(os.path.join(path, '*g'))\n",
    "    for fl in files:\n",
    "        image = cv2.imread(fl)\n",
    "        image = image.astype(np.float32)\n",
    "        image = np.multiply(image, 1.0 / 255.0)\n",
    "        images.append(image)\n",
    "        label = np.zeros(class_num)\n",
    "        label[class_index] = 1.0\n",
    "        labels.append(label)\n",
    "    return images,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in image data from sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_disk(input_queue):\n",
    "    \"\"\"Consumes a single filename and label as a ' '-delimited string.\n",
    "    Args:\n",
    "      filename_and_label_tensor: A scalar string tensor.\n",
    "    Returns:\n",
    "      Two tensors: the decoded image, and the string label.\n",
    "    \"\"\"\n",
    "    label = input_queue[1]\n",
    "    file_contents = tf.read_file(input_queue[0])\n",
    "    example = tf.image.decode_png(file_contents, channels=3)\n",
    "    return example, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/caitdreisbach/Downloads/imagesML/train/contempt', '/Users/caitdreisbach/Downloads/imagesML/train/fear', '/Users/caitdreisbach/Downloads/imagesML/train/surprise', '/Users/caitdreisbach/Downloads/imagesML/train/sadness', '/Users/caitdreisbach/Downloads/imagesML/train/neutral', '/Users/caitdreisbach/Downloads/imagesML/train/happiness', '/Users/caitdreisbach/Downloads/imagesML/train/anger', '/Users/caitdreisbach/Downloads/imagesML/train/disgust']\n"
     ]
    }
   ],
   "source": [
    "#get labeled directories into a list\n",
    "directory_list = list()\n",
    "for root, dirs, files in os.walk(\"/Users/caitdreisbach/Downloads/imagesML/train\", topdown=False):\n",
    "    for name in dirs:\n",
    "        directory_list.append(os.path.join(root, name))\n",
    "\n",
    "print(directory_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove path prior to file name\n",
    "import ntpath\n",
    "ntpath.basename(\"/Users/caitdreisbach/Downloads/imagesML/train/\")\n",
    "\n",
    "def path_leaf(path):\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "paths = ['/Users/caitdreisbach/Downloads/imagesML/train/contempt', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/fear', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/surprise', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/sadness', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/neutral', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/happiness', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/anger', \n",
    "         '/Users/caitdreisbach/Downloads/imagesML/train/disgust']\n",
    "labels = [path_leaf(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contempt', 'fear', 'surprise', 'sadness', 'neutral', 'happiness', 'anger', 'disgust']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images\n",
      "['/Users/caitdreisbach/Downloads/imagesML/train/contempt', '/Users/caitdreisbach/Downloads/imagesML/train/fear', '/Users/caitdreisbach/Downloads/imagesML/train/surprise', '/Users/caitdreisbach/Downloads/imagesML/train/sadness', '/Users/caitdreisbach/Downloads/imagesML/train/neutral', '/Users/caitdreisbach/Downloads/imagesML/train/happiness', '/Users/caitdreisbach/Downloads/imagesML/train/anger', '/Users/caitdreisbach/Downloads/imagesML/train/disgust']\n",
      "Finished reading images\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b668043aa74f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Faces'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJIAAAD8CAYAAACchf2kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWCSksEhIYZGQwiIhhcX/AZ8AWKJF4MC9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b5cd518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# load images\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "print('Reading images')\n",
    "\n",
    "#images,labels=read_images('/Users/caitdreisbach/Downloads/imagesML/train',images,labels,2,0)\n",
    "rootDir = '/Users/caitdreisbach/Downloads/imagesML/train'\n",
    "\n",
    "#get labeled directories into a list\n",
    "directory_list = list()\n",
    "for root, dirs, files in os.walk(\"/Users/caitdreisbach/Downloads/imagesML/train\", topdown=False):\n",
    "    for name in dirs:\n",
    "        directory_list.append(os.path.join(root, name))\n",
    "\n",
    "print(directory_list)\n",
    "\n",
    "# traverse root directory, and list directories as dirs and files as files\n",
    "#for root, dirs, files in os.walk(rootDir):\n",
    "#    path = root.split(os.sep)\n",
    "#    print((len(path) - 1) * '---', os.path.basename(root))\n",
    "#    for file in files:\n",
    "#        print(len(path) * '---', file)\n",
    "        \n",
    "        \n",
    "print('Finished reading images') \n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Create dataset\n",
    "train = DataSet(images, labels)\n",
    "\n",
    "# parameters\n",
    "model_dir=\"model_MonaLisa\"\n",
    "max_itr=1000\n",
    "image_size=64\n",
    "\n",
    "# Inspect Data: Show first image of each type\n",
    "fig = plt.figure(figsize=(4, 4)) \n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "plt.imshow(images[0])\n",
    "ax.set_title('Faces')  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create computational graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.ops.nn' has no attribute 'softmax_cross_entropy_with_logits_v2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-91051735af67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_conv_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.ops.nn' has no attribute 'softmax_cross_entropy_with_logits_v2'"
     ]
    }
   ],
   "source": [
    "# ******** Cell - 1 ***********\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():   \n",
    "    # Create the computational graph:\n",
    "    # Placeholders:\n",
    "    x = tf.placeholder(tf.float32, [None, image_size,image_size,3],name=\"x\")\n",
    "    y = tf.placeholder(tf.float32, [None, 2],name=\"y\")\n",
    "        \n",
    "    # Build the graph for the deep net\n",
    "    y_conv_out,keep_prob = deepnn(x)\n",
    "    \n",
    "    # Add loss, accuracy and optimization\n",
    "    probs=tf.nn.softmax(logits=y_conv_out, name=\"probs_op\")\n",
    "    \n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y,logits=y_conv_out)\n",
    "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('adam_optimizer'):\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y_conv_out, 1), tf.argmax(y, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction, name=\"acc_op\")\n",
    "        \n",
    "    # Add a saver to save the trained model\n",
    "    saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** Cell - 2 ***********\n",
    "# Reset graph: useful for multiple runs (e.g., parameter tuning, CV, etc.)\n",
    "tf.reset_default_graph()  \n",
    "# Create session to execute graph\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(max_itr):\n",
    "        # Save a version of the model every 100 iterations\n",
    "        if i>0 and i%100==0:\n",
    "            saver.save(sess, \"./%s/model\"%(model_dir), global_step=i)\n",
    "            \n",
    "        # Get a batch of 32 images    \n",
    "        x_batch, y_true_batch = train.next_batch(32) \n",
    "        \n",
    "        # Get training accuracy on the batch\n",
    "        if i>0 and i % 100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: x_batch , y: y_true_batch, keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "            \n",
    "        # Train\n",
    "        train_step.run(feed_dict={x: x_batch, y: y_true_batch, keep_prob: 0.5})\n",
    "    \n",
    "    # save final model\n",
    "    save_path = saver.save(sess, \"./%s/model\"%(model_dir), global_step=max_itr)\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    # Get training accuracy on all training images\n",
    "    train_accuracy=accuracy.eval(feed_dict={x: images,y: labels, keep_prob: 1.0 })\n",
    "    print (\"Training Accuracy: %10.5f\"%train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing images\n",
    "test_images = []   \n",
    "test_labels = []   \n",
    "\n",
    "print('Reading testing images')\n",
    "test_images,test_labels=read_images('jellyfish_valid',test_images,test_labels,2,0)\n",
    "test_images,test_labels=read_images('bullfrog_valid',test_images,test_labels,2,1)\n",
    "print('Finished reading testing images') \n",
    "\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** Cell - 3 ***********\n",
    "# Test model on testing data\n",
    "with tf.Session() as sess:    \n",
    "    saver = tf.train.import_meta_graph('./%s/model-%s.meta'%(model_dir,max_itr))\n",
    "    saver.restore(sess,tf.train.latest_checkpoint(model_dir))\n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "     \n",
    "    x = graph.get_tensor_by_name(\"x:0\")\n",
    "    y = graph.get_tensor_by_name(\"y:0\")\n",
    "    keep_prob= graph.get_tensor_by_name(\"dropout/keep_prob:0\") \n",
    "    accuracy = graph.get_tensor_by_name(\"accuracy/acc_op:0\")\n",
    "    \n",
    "    acc_val=accuracy.eval(feed_dict={x: test_images,y: test_labels, keep_prob: 1.0 })\n",
    "    \n",
    "    print (\"Testing Accuracy: %10.5f\"%acc_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (more challenging): Bull Frog VS. Tailed Frog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "print('Reading images')\n",
    "images,labels=read_images('tailed frog_train/images',images,labels,2,0)\n",
    "images,labels=read_images('bullfrog_train/images',images,labels,2,1)\n",
    "print('Finished reading images')\n",
    "\n",
    "# Create dataset\n",
    "train = DataSet(np.array(images), np.array(labels))\n",
    "\n",
    "# parameters\n",
    "model_dir=\"model_bull_tailed_frog\"\n",
    "max_itr=1000\n",
    "image_size=64\n",
    "\n",
    "# Inspect Data: Show first image of each type\n",
    "fig = plt.figure(figsize=(4, 4)) \n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "plt.imshow(images[0])\n",
    "ax.set_title('Tailed Frog')  \n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "plt.imshow(images[501])\n",
    "ax.set_title('Bull Frog')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run Cells 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing images and test model\n",
    "test_images=[]\n",
    "test_labels=[]\n",
    "print('Reading images')\n",
    "test_images,test_labels=read_images('tailed frog_valid',test_images,test_labels,2,0)\n",
    "test_images,test_labels=read_images('bullfrog_valid',test_images,test_labels,2,1)\n",
    "print('Finished reading images') \n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Run Cell 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to improve our ConvNet model\n",
    "# Approach: Try larger filters, more filters & Train for more iterations\n",
    "max_itr = 3000\n",
    "def deepnn(x):\n",
    "    # x is 64x64x3\n",
    "    \n",
    "    # First Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([11, 11, 3, 16],'conv_w')\n",
    "        b_conv1 = bias_variable([16],'conv_b')\n",
    "        h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1,name='h_conv_op')\n",
    "        h_pool1 = max_pool_2x2(h_conv1,'h_pool_op')\n",
    "    # output: 32x32x16\n",
    " \n",
    "    # Second Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([11, 11, 16, 32],'conv_w')\n",
    "        b_conv2 = bias_variable([32],'conv_b')\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2,name='h_conv_op')\n",
    "        h_pool2 = max_pool_2x2(h_conv2,'h_pool_op')\n",
    "    # output: 16x16x16\n",
    "\n",
    "    # Third Conv, Relu and 2x2 max-pooling layers\n",
    "    with tf.name_scope('conv3'):\n",
    "        W_conv3 = weight_variable([5, 5, 32, 64],'conv_w')\n",
    "        b_conv3 = bias_variable([64],'conv_b')\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3,name='h_conv_op')\n",
    "        h_pool3 = max_pool_2x2(h_conv3,'h_pool_op')\n",
    "    # output: 8x8x32\n",
    "\n",
    "    # First Fully Connected Layer\n",
    "    with tf.name_scope('fc'):\n",
    "        # Flatten output from previous layer\n",
    "        h_pool3_flat = tf.reshape(h_pool3, [-1, 8*8*64])\n",
    "        # Fully connected + Relu\n",
    "        # input: flat (8x8x32) = 1x2048\n",
    "        W_fc1 = weight_variable([8 * 8* 64, 1024],'fc_w')\n",
    "        b_fc1 = bias_variable([1024],'fc_b')\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "    # output: 1x1024\n",
    "    \n",
    "    # Dropout\n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32,name=\"keep_prob\")\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "    # Output Softmax Layer: Map the 1024 features to 2 classes, one for each class type\n",
    "    with tf.name_scope('sm'):\n",
    "        W_sm = weight_variable([1024, 2],'sm_w')\n",
    "        b_sm = bias_variable([2],'sm_b')\n",
    "        y_conv_out = tf.add(tf.matmul(h_fc1_drop, W_sm), b_sm, name=\"y_conv_op\")\n",
    " \n",
    "    return y_conv_out,keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Cells 1, 2 and 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet block definition\n",
    "<img src=\"resNetBlock.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Source: https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py\n",
    "################################################################################\n",
    "def _building_block_v1(inputs, filters, training, projection_shortcut, strides,\n",
    "                       data_format):\n",
    "  \"\"\"A single block for ResNet v1, without a bottleneck.\n",
    "  Convolution then batch normalization then ReLU as described by:\n",
    "    Deep Residual Learning for Image Recognition\n",
    "    https://arxiv.org/pdf/1512.03385.pdf\n",
    "    by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, Dec 2015.\n",
    "  Args:\n",
    "    inputs: A tensor of size [batch, channels, height_in, width_in] or\n",
    "      [batch, height_in, width_in, channels] depending on data_format.\n",
    "    filters: The number of filters for the convolutions.\n",
    "    training: A Boolean for whether the model is in training or inference\n",
    "      mode. Needed for batch normalization.\n",
    "    projection_shortcut: The function to use for projection shortcuts\n",
    "      (typically a 1x1 convolution when downsampling the input).\n",
    "    strides: The block's stride. If greater than 1, this block will ultimately\n",
    "      downsample the input.\n",
    "    data_format: The input format ('channels_last' or 'channels_first').\n",
    "  Returns:\n",
    "    The output tensor of the block; shape should match inputs.\n",
    "  \"\"\"\n",
    "  shortcut = inputs\n",
    "\n",
    "  if projection_shortcut is not None:\n",
    "    shortcut = projection_shortcut(inputs)\n",
    "    shortcut = batch_norm(inputs=shortcut, training=training,\n",
    "                          data_format=data_format)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=strides,\n",
    "      data_format=data_format)\n",
    "  inputs = batch_norm(inputs, training, data_format)\n",
    "  inputs = tf.nn.relu(inputs)\n",
    "\n",
    "  inputs = conv2d_fixed_padding(\n",
    "      inputs=inputs, filters=filters, kernel_size=3, strides=1,\n",
    "      data_format=data_format)\n",
    "  inputs = batch_norm(inputs, training, data_format)\n",
    "  inputs += shortcut\n",
    "  inputs = tf.nn.relu(inputs)\n",
    "\n",
    "return inputs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
